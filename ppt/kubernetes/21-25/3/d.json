[
    {
      "title": "Introduction to Scaling Pods in Kubernetes",
      "content": "Scaling in Kubernetes allows you to increase or decrease the number of pod replicas for better performance and resource utilization.\n\nThis is especially useful for handling more traffic or reducing costs when demand is low.",
      "slide_type": "text"
    },
    {
      "title": "Why Scale Pods?",
      "content": "- Handle increased traffic\n- Improve availability and fault tolerance\n- Optimize resource usage and cost\n- Maintain consistent performance",
      "slide_type": "text"
    },
    {
      "title": "Manual Scaling with kubectl",
      "content": "You can manually scale the number of pods using the `kubectl scale` command:\n\n```bash\nkubectl scale deployment <deployment-name> --replicas=<number>\n\n# Example:\nkubectl scale deployment my-app --replicas=5\n```",
      "slide_type": "code"
    },
    {
      "title": "Autoscaling with Horizontal Pod Autoscaler (HPA)",
      "content": "Kubernetes offers **Horizontal Pod Autoscaler (HPA)** to automatically adjust the number of pods based on CPU or custom metrics.\n\nKey components:\n- Target resource (e.g., Deployment)\n- Metrics (e.g., CPU usage)\n- Min and max replicas",
      "slide_type": "text"
    },
    {
      "title": "Creating a Horizontal Pod Autoscaler",
      "content": "```bash\nkubectl autoscale deployment <deployment-name> \\\n  --cpu-percent=<target-cpu-utilization> \\\n  --min=<min-pods> --max=<max-pods>\n\n# Example:\nkubectl autoscale deployment my-app --cpu-percent=50 --min=1 --max=10\n```",
      "slide_type": "code"
    },
    {
      "title": "Comparison: Manual vs. Autoscaling",
      "content": [
        {
          "Method": "Manual Scaling",
          "Trigger": "User action",
          "Use Case": "Predictable traffic patterns"
        },
        {
          "Method": "Autoscaling (HPA)",
          "Trigger": "Resource metrics (e.g., CPU)",
          "Use Case": "Variable or unpredictable traffic"
        }
      ],
      "slide_type": "table"
    },
    {
      "title": "Best Practices for Scaling",
      "content": "- Always set resource requests/limits in pod specs\n- Monitor metrics with tools like Prometheus + Grafana\n- Test your app's performance under load\n- Combine HPA with Cluster Autoscaler for full flexibility",
      "slide_type": "text"
    },
    {
      "title": "Quick Challenge: Test Your Understanding",
      "content": "### 1. What command do you use to scale a deployment to 4 replicas manually?\n\n### 2. What metric does HPA typically use to scale pods?\n\n### 3. True or False: Autoscaling works without setting CPU requests on pods.\n\n### 4. What is the benefit of using HPA over manual scaling?",
      "slide_type": "text"
    },
    {
      "title": "Solutions: Quick Challenge",
      "content": "```markdown\n1. `kubectl scale deployment <deployment-name> --replicas=4`\n\n2. CPU utilization (or custom metrics if configured)\n\n3. False â€” HPA requires resource requests to be defined to calculate metrics.\n\n4. HPA adjusts pod count automatically based on load, reducing the need for manual intervention and improving responsiveness.\n```",
      "slide_type": "code"
    }
  ]
  